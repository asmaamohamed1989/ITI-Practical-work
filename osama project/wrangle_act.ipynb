{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling: *WeRateDogs* Twitter Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tweepy\n",
    "import requests\n",
    "import re\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import os\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1- Data Gathering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The twitter archive and image prediction gathering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twitter archive data \n",
    "archive = pd.read_csv('twitter-archive-enhanced.csv')\n",
    "\n",
    "# Image prediction data\n",
    "\n",
    "image_predictions = pd.read_csv('https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>in_reply_to_status_id</th>\n",
       "      <th>in_reply_to_user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "      <th>retweeted_status_id</th>\n",
       "      <th>retweeted_status_user_id</th>\n",
       "      <th>retweeted_status_timestamp</th>\n",
       "      <th>expanded_urls</th>\n",
       "      <th>rating_numerator</th>\n",
       "      <th>rating_denominator</th>\n",
       "      <th>name</th>\n",
       "      <th>doggo</th>\n",
       "      <th>floofer</th>\n",
       "      <th>pupper</th>\n",
       "      <th>puppo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>892420643555336193</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-08-01 16:23:56 +0000</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>This is Phineas. He's a mystical boy. Only eve...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://twitter.com/dog_rates/status/892420643...</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>Phineas</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>892177421306343426</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-08-01 00:17:27 +0000</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>This is Tilly. She's just checking pup on you....</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://twitter.com/dog_rates/status/892177421...</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>Tilly</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>891815181378084864</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-07-31 00:18:03 +0000</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>This is Archie. He is a rare Norwegian Pouncin...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://twitter.com/dog_rates/status/891815181...</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>Archie</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id  in_reply_to_status_id  in_reply_to_user_id  \\\n",
       "0  892420643555336193                    NaN                  NaN   \n",
       "1  892177421306343426                    NaN                  NaN   \n",
       "2  891815181378084864                    NaN                  NaN   \n",
       "\n",
       "                   timestamp  \\\n",
       "0  2017-08-01 16:23:56 +0000   \n",
       "1  2017-08-01 00:17:27 +0000   \n",
       "2  2017-07-31 00:18:03 +0000   \n",
       "\n",
       "                                              source  \\\n",
       "0  <a href=\"http://twitter.com/download/iphone\" r...   \n",
       "1  <a href=\"http://twitter.com/download/iphone\" r...   \n",
       "2  <a href=\"http://twitter.com/download/iphone\" r...   \n",
       "\n",
       "                                                text  retweeted_status_id  \\\n",
       "0  This is Phineas. He's a mystical boy. Only eve...                  NaN   \n",
       "1  This is Tilly. She's just checking pup on you....                  NaN   \n",
       "2  This is Archie. He is a rare Norwegian Pouncin...                  NaN   \n",
       "\n",
       "   retweeted_status_user_id retweeted_status_timestamp  \\\n",
       "0                       NaN                        NaN   \n",
       "1                       NaN                        NaN   \n",
       "2                       NaN                        NaN   \n",
       "\n",
       "                                       expanded_urls  rating_numerator  \\\n",
       "0  https://twitter.com/dog_rates/status/892420643...                13   \n",
       "1  https://twitter.com/dog_rates/status/892177421...                13   \n",
       "2  https://twitter.com/dog_rates/status/891815181...                12   \n",
       "\n",
       "   rating_denominator     name doggo floofer pupper puppo  \n",
       "0                  10  Phineas  None    None   None  None  \n",
       "1                  10    Tilly  None    None   None  None  \n",
       "2                  10   Archie  None    None   None  None  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "archive.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>jpg_url</th>\n",
       "      <th>img_num</th>\n",
       "      <th>p1</th>\n",
       "      <th>p1_conf</th>\n",
       "      <th>p1_dog</th>\n",
       "      <th>p2</th>\n",
       "      <th>p2_conf</th>\n",
       "      <th>p2_dog</th>\n",
       "      <th>p3</th>\n",
       "      <th>p3_conf</th>\n",
       "      <th>p3_dog</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>666020888022790149</td>\n",
       "      <td>https://pbs.twimg.com/media/CT4udn0WwAA0aMy.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>Welsh_springer_spaniel</td>\n",
       "      <td>0.465074</td>\n",
       "      <td>True</td>\n",
       "      <td>collie</td>\n",
       "      <td>0.156665</td>\n",
       "      <td>True</td>\n",
       "      <td>Shetland_sheepdog</td>\n",
       "      <td>0.061428</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>666029285002620928</td>\n",
       "      <td>https://pbs.twimg.com/media/CT42GRgUYAA5iDo.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>redbone</td>\n",
       "      <td>0.506826</td>\n",
       "      <td>True</td>\n",
       "      <td>miniature_pinscher</td>\n",
       "      <td>0.074192</td>\n",
       "      <td>True</td>\n",
       "      <td>Rhodesian_ridgeback</td>\n",
       "      <td>0.072010</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>666033412701032449</td>\n",
       "      <td>https://pbs.twimg.com/media/CT4521TWwAEvMyu.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>German_shepherd</td>\n",
       "      <td>0.596461</td>\n",
       "      <td>True</td>\n",
       "      <td>malinois</td>\n",
       "      <td>0.138584</td>\n",
       "      <td>True</td>\n",
       "      <td>bloodhound</td>\n",
       "      <td>0.116197</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id                                          jpg_url  \\\n",
       "0  666020888022790149  https://pbs.twimg.com/media/CT4udn0WwAA0aMy.jpg   \n",
       "1  666029285002620928  https://pbs.twimg.com/media/CT42GRgUYAA5iDo.jpg   \n",
       "2  666033412701032449  https://pbs.twimg.com/media/CT4521TWwAEvMyu.jpg   \n",
       "\n",
       "   img_num                      p1   p1_conf  p1_dog                  p2  \\\n",
       "0        1  Welsh_springer_spaniel  0.465074    True              collie   \n",
       "1        1                 redbone  0.506826    True  miniature_pinscher   \n",
       "2        1         German_shepherd  0.596461    True            malinois   \n",
       "\n",
       "    p2_conf  p2_dog                   p3   p3_conf  p3_dog  \n",
       "0  0.156665    True    Shetland_sheepdog  0.061428    True  \n",
       "1  0.074192    True  Rhodesian_ridgeback  0.072010    True  \n",
       "2  0.138584    True           bloodhound  0.116197    True  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_predictions.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading and saving the image prediction data using Requests\n",
    "\n",
    "url = 'https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv'\n",
    "r = requests.get(url)\n",
    "file_name = url.split('/')[-1]\n",
    "if not os.path.isfile(file_name):\n",
    "    with open(file_name, 'wb') as f:\n",
    "        f.write(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitter API Data for the favourites and retweets counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'config'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-ec106a90b3ed>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mconsumer_key\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'7IRTUjDbsW5mAemizSNq4A94a'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mconsumer_secret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'X9uGNBt8z4brGsfzSm98QUnWpMoNkYMx9WCgPeoTdRufFkAubv'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0maccess_token\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'1002855692-yfhGLsMm4mD2096j3Bd63yBIxY0e8agnbFDPKoG'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0maccess_secret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'kw86gw3YihmsWFLkj2u8iTdIyBFyWxXv2WXmfzPIyASIY'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'config'"
     ]
    }
   ],
   "source": [
    "import config \n",
    "consumer_key = '7IRTUjDbsW5mAemizSNq4A94a'\n",
    "consumer_secret = 'X9uGNBt8z4brGsfzSm98QUnWpMoNkYMx9WCgPeoTdRufFkAubv'\n",
    "access_token = '1002855692-yfhGLsMm4mD2096j3Bd63yBIxY0e8agnbFDPKoG'\n",
    "access_secret = 'kw86gw3YihmsWFLkj2u8iTdIyBFyWxXv2WXmfzPIyASIY'\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_secret)\n",
    "\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'api' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-c84aac6aabf3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Experimenting to extract one tweet's id information\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mexp_tweet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mapi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marchive\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtweet_id\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtweet_mode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'extended'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mcontent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexp_tweet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_json\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mcontent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'api' is not defined"
     ]
    }
   ],
   "source": [
    "# Experimenting to extract one tweet's id information\n",
    "\n",
    "exp_tweet = api.get_status(archive.tweet_id[1000], tweet_mode = 'extended')\n",
    "content = exp_tweet._json\n",
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the keys of each tweet\n",
    "content.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the retweet and favorite counts\n",
    "content['retweet_count'] , content['favorite_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#investigating the user information\n",
    "content['user'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content['user']['followers_count'], content['user']['location']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a file for the additional tweets text data\n",
    "if not os.path.isfile('tweet_json.txt'):\n",
    "    with open ('tweet_json.txt', 'w') as file:\n",
    "        for tweet_id in archive['tweet_id']:\n",
    "            try:\n",
    "                status = api.get_status(tweet_id, wait_on_rate_limit=True,  wait_on_rate_limit_notify=True, tweet_mode = 'extended')\n",
    "                json.dump(status._json, file)\n",
    "                file.write('\\n')\n",
    "            except Exception as e:\n",
    "                print(\"Error on tweet id {}\".format(tweet_id) + \";\" + str(e))\n",
    "                errors.append(tweet_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = []\n",
    "\n",
    "with open('tweet_json.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        tweet = json.loads(line)\n",
    "        tweets.append(tweet)\n",
    "        \n",
    "tweets[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_df = pd.DataFrame()\n",
    "\n",
    "api_df['id'] = list(map(lambda tweet: tweet['id'], tweets))\n",
    "api_df['retweet_count'] = list(map(lambda tweet: tweet['retweet_count'], tweets))\n",
    "api_df['favorite_count'] = list(map(lambda tweet: tweet['favorite_count'], tweets))\n",
    "api_df['followers_count'] = list(map(lambda tweet: tweet['user']['followers_count'], tweets))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resources:\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Assessment\n",
    "\n",
    "That's where the inspection of our collected data sets from both the ***Quality*** and ***Tidiness*** prespectives will be conducted.\n",
    "\n",
    "- Data quality dimensions help guide the thought process while assessing and also cleaning. The four main data quality dimensions are:\n",
    "\n",
    "    -  Completeness: do we have all of the records that we should? Do we have missing records or not? Are there specific rows, columns, or cells missing?\n",
    "    -  Validity: we have the records, but they're not valid, i.e., they don't conform to a defined schema. A schema is a defined set of rules for data. These rules can be real-world constraints (e.g. negative height is impossible) and table-specific constraints (e.g. unique key constraints in tables).\n",
    "    -  Accuracy: inaccurate data is wrong data that is valid. It adheres to the defined schema, but it is still incorrect. Example: a patient's weight that is 5 lbs too heavy because the scale was faulty.\n",
    "    -  Consistency: inconsistent data is both valid and accurate, but there are multiple correct ways of referring to the same thing. Consistency, i.e., a standard format, in columns that represent the same data across tables and/or within tables is desired."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dimensions of the 3 datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"archive: \\n\" + str(archive.shape) + \"\\n\" + 'image_predictions: \\n' + str(image_predictions.shape) + '\\n' + 'api_df: \\n' + str(api_df.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. The Twitter *`archive`* table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "archive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are all or most of dogs classified\n",
    "print(archive.doggo.value_counts(), '\\n') \n",
    "print(archive.floofer.value_counts(),'\\n')\n",
    "print(archive.pupper.value_counts(),'\\n')\n",
    "print(archive.puppo.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(archive.loc[:, 'doggo':\"puppo\"] != 'None') .sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Is the classification correct and mutually exclusive\n",
    "# https://stackoverflow.com/questions/42461086/subset-pandas-dataframe-using-values-from-two-columns\n",
    "nonunique_stage = archive[(archive['doggo'] != 'None') & (archive['pupper'] != 'None')]\n",
    "nonunique_stage.iloc[:, -4:].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the most frequent names\n",
    "archive.name.value_counts().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the name column and in relation to the text\n",
    "archive.loc[:,['text', 'name']].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive.loc[:,['text', 'name']][(archive['name'] == 'None')].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are all names capitalized.\n",
    "archive.name.str.islower().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seems that the way the names are captured is by \n",
    "# scraping the word after the \"this is\" or the word \"meet\"\n",
    "archive.loc[[1049] ,['text', 'name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some texts contain more than one name:\n",
    "archive.loc[[1007] ,['text', 'name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some names weren't successfully captured from the text\n",
    "archive.loc[[391,1501] ,['text', 'name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the ratings values\n",
    "archive.loc[:, ['rating_numerator', 'rating_denominator']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive.rating_denominator.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive.rating_numerator.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive.source.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. The *`image_predictions`* table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_predictions.sample(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_predictions.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_predictions[(image_predictions['p1_dog'] == False) & \n",
    "                  (image_predictions['p2_dog'] == False) &\n",
    "                  (image_predictions['p3_dog'] == False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_predictions.p1.value_counts().head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_predictions.p2.value_counts().head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_predictions.p3.value_counts().head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_predictions.loc[:,['p1_conf', 'p2_conf', 'p3_conf']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III. The twitter *`api_df`* table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_df.sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_df.loc[:, ['retweet_count', 'favorite_count', 'followers_count']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Warngling Scope of work\n",
    "As per the project requirements; only original ratings (*no retweets*) that ***have images** should be included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive.retweeted_status_id.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seems that the way the names are captured is by \n",
    "# scraping the word after the strings \"this is\" or  \"meet\"\n",
    "archive.loc[[1049] ,['text', 'name']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessment summary\n",
    "\n",
    "### Quality aspects\n",
    "\n",
    "#### *`archive`* table\n",
    "\n",
    "+ **Data types(consistency issues):** \n",
    "    -  All timestamps are object type\n",
    "    - All tweet_ids are integers\n",
    "    - Inconsistent representation of null values as \"None\" strings in the (name, doggo, floofer, pupper, puppo) columns.\n",
    "    - The variables (in_reply_to_status_id,  in_reply_to_user_id, 'retweeted_status_id', 'retweeted_status_user_id', 'retweeted_status_timestamp') will not be included in our analysis, their main use will be to identify retweets(drop retweeted_status_id with non-null values) and replies (drop in_reply_to_status_id with non-null values)to exclude them. \n",
    "    \n",
    " \n",
    "- **completeness isues:** \n",
    "    -  In the name column, some names weren't successfully extracted from the text e.g. idexes 391,1501 while others contain more than one name in the text while only one was extracted e.g. index 1007.\n",
    "    -  Missing entries in expanded_urls. \n",
    "    - columns (in_reply_to_status_id,  in_reply_to_user_id, 'retweeted_status_id', 'retweeted_status_user_id', 'retweeted_status_timestamp', 'doggo', 'floofer', 'pupper', 'puppo') include a large number of missimg values.\n",
    "    - Some tweets may not include any image; those should be deleted (note the discrepancy in the number of tweets between the `archive` dataset and the `image_prediction` one)\n",
    "    - Some tweets are actually retweets and replies not original tweets that have to be deleted as per the data wrangling scope mandated by the project specification. (Note: those tweets should be removed from the three tables in hand)\n",
    "\n",
    "- **Accuracy issues:**\n",
    "    -  erroneous names extracted like the second most frequent name \"the letter a\". Those needs to be rendered null.\n",
    "    - Incorrect and weird values of the rating_numerator which has a maximum of 1776.\n",
    "    - The same holds for the rating_denominator with illogical maximum of 170\n",
    "\n",
    "#### *`image_predictions`* table\n",
    "- Non-descriptive columns' names\n",
    "- Inconsistent capitalization for the prdicted breeds(p1, p2, p3 )\n",
    "- In general the total number of records (2075 instead of 2356) indicates the presence of some tweets without images which we need to exclude.\n",
    "\n",
    "#### *`api_df`* table\n",
    "- In general the total number of records (2236) indicates the presence of some tweets without relevant retweet and favorite counts. \n",
    "\n",
    "### Tidiness aspects:\n",
    "\n",
    "#### *`archive`* table\n",
    "- values are column names(doggo, floofer, pupper, puppo); they better be combined in one column names \"dog_stage\"\n",
    "\n",
    "#### *`image_predictions`* table\n",
    "- values are column names(p1,p2,p3) which are all breed predictions\n",
    "\n",
    "#### *`api_df`* table\n",
    "- This isn't considered an observational unit to have its own table; so, it should be merged to the *`archive`* table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copying the datasets \n",
    "archive_clean = archive.copy()\n",
    "image_clean = image_predictions.copy()\n",
    "api_clean = api_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Addressing the missing values in the archive dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. In the *`name`* column:**\n",
    " >   -  some names weren't successfully extracted from the text e.g. idexes 391, 1501 \n",
    "     -  while others contain more than one name in the text while only one was extracted e.g. index 1007. *(This will not be addressed due as a tradeoff with keeping the dataset intact and to not jeoparadize its structure for fixing sucu a rare issue)*\n",
    "     -  erroneous names extracted like the second most frequent name \"the letter a\". Those needs to be rendered null.\n",
    "     -  Inconsistent representation of null values as \"None\" strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define\n",
    ">-  I will firstly fix the visually detected rows manually. Dealing with the names with the string \"a\" value.  \n",
    "-  Scrutinizing the texts that were used to extract the \"a\" names; i noticed a pettern in many of them that they contain the name of the dog after the word \"named\"; The following code will extract the name if exists and replace the value \"a\" with it, otherwise will set the \"a\" to \"None\".\n",
    "-  Replacing the 'None' string with pythonic null values\n",
    "\n",
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The visually detected rows with names not correctly extracted\n",
    "archive_clean.loc[[archive_clean.index[391],archive_clean.index[1501] ], ['text', 'name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# archive_clean.loc[archive_clean.dog_stage == \"\", 'dog_stage'] = np.nan\n",
    "archive_clean.loc[391, 'name'] = 'Dew'\n",
    "archive_clean.loc[1501, 'name'] = 'Teddy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Many texts have the names after the word 'named'\n",
    "archive_clean[archive_clean['name'] == 'a'][['text', 'name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#before dealing with the issue in hand let's investigate some names that were erroneously extracted as the letter \"a\"\n",
    "#archive_clean.name[1853] == 'a' while it's 'Wylie'\n",
    "#archive_clean.name[1955] == 'a' while it's 'Kip'\n",
    "#archive_clean.name[2034] == 'a'while it's  'Jacob'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compiling a pattern to catch the names and experimenting its validity\n",
    "pattern = re.compile('(?:name(?:d)?)\\s{1}(?:is\\s)?([A-Za-z]+)')\n",
    "re.findall(pattern, 'This is a Dasani Kingfisher from Maine. His name is Daryl. Daryl doesn''t like being swallowed by a panda. 8/10 https://t.co/jpaeu6LNmW')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crafting a function to fix the name =='a'\n",
    "    # column and replace it with either the name or string 'None'\n",
    "    \n",
    "def fix_a_names(row):\n",
    "    if row['name'] == \"a\" or row['name'] == 'an':\n",
    "        pattern = re.compile('(?:name(?:d)?)\\s{1}(?:is\\s)?([A-Za-z]+)')\n",
    "        try:\n",
    "            new = re.findall(pattern, row['text'])[0]\n",
    "            row['name'] = row['name'].replace('a', new)\n",
    "            row['name'] = row['name'].replace('an', new)\n",
    "        except Exception:\n",
    "            new = \"None\"\n",
    "            row['name'] = row['name'].replace('a', new)\n",
    "            row['name'] = row['name'].replace('an', new)\n",
    "        return row['name']\n",
    "    else:\n",
    "        return row['name']\n",
    "archive_clean['name'] = archive_clean.apply(fix_a_names, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resources:\n",
    "[return string with first match Regex](https://stackoverflow.com/questions/38579725/return-string-with-first-match-regex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_clean['name'] = archive_clean.name.replace({'None':None})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the manually fixed names\n",
    "archive_clean.loc[[archive_clean.index[391],archive_clean.index[1501] ], ['text', 'name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing one of the previously named 'a' while having a name\n",
    "archive_clean.iloc[[1853],list(archive_clean).index('name')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the three of the name == 'a' entries\n",
    "archive_clean.loc[[archive_clean.index[1853], archive_clean.index[1955], archive_clean.index[2034] ], ['text', 'name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there is no 'a' names\n",
    "'a' in archive_clean['name'] , 'an' in archive_clean['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the conversion on 'None' strings to Nan\n",
    "archive_clean.name.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'None' in archive_clean['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_clean.name.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. For the *`expanded`_url* column**\n",
    ">  There are 59 missing values out of 2356. Firstly this figure is a pretty low and by little research i found out that those missing values are primarily tweets without photos that shoud not be taken into account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define\n",
    "Given that the Na values are tweets with no images, dropping the missing values of this variable is an appropriate decision using the dropna method on the archive_clean dataframe\n",
    "\n",
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_clean.expanded_urls.value_counts(dropna = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_clean.dropna(subset=['expanded_urls'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resources:\n",
    "[How to drop a row whose particular column is empty/NaN?](https://stackoverflow.com/questions/46091924/python-how-to-drop-a-row-whose-particular-column-is-empty-nan)\n",
    "\n",
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_clean.expanded_urls.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. For the *`doggo, floofer, pupper, puppo`* columns missing values**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define \n",
    "To later fix the tidiness issue of those values disguised as variables, addressing the \"None\" strings better be fixed first by replacing all the \"None\" with the empty string \"\".\n",
    "\n",
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_clean.iloc[:, -4:  ].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_clean.iloc[:, -4:  ] = archive_clean.iloc[:, -4:  ].replace('None','')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's the actual representation of the values of the four columns\n",
    "archive_clean.iloc[:, -4:  ].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_clean.doggo.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Addressing theTidiness issues:\n",
    "1. The ***`archive`*** dataset. Values represented as variables for doggo, floofer, pupper and puppo columns, all are dog stages.\n",
    "2. The ***`image_predictions`*** dataset also has values represented as variables:\n",
    ">    -  For  p1, p2, p3, they contain dog breed predictions.\n",
    ">    -  The variables (p1_conf, p2_conf, p3_conf) representing one variable for confidence level of the predictions.\n",
    ">    - Finally, the columns (p1_dog, p2_dog, p3_dog) represents a boolean variable if the predicted animal was a dog or not\n",
    "    \n",
    "3. The ***`api_df`*** dataset better be merged with the archive datasert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. The archive dataset. Values represented as variables for doggo, floofer, pupper and puppo columns, all are dog stages.**\n",
    "#### Define\n",
    "Addressing the tidiness issue in the `archive` dataset by combining the aforementioned 4 columns into one column through simple pandas series addition opertaion. The resulting column quality issues aill be addressed later on.\n",
    "\n",
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_clean['dog_stage'] = archive_clean.doggo + archive_clean.floofer + archive_clean.pupper + archive_clean.puppo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_clean.drop(columns=['doggo', 'floofer', 'pupper', 'puppo'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_clean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_clean.dog_stage.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The Original figures were:\n",
    "    -  doggo      97 \n",
    "    -  floofer    10 \n",
    "    -  pupper     257\n",
    "    -  puppo      30 \n",
    "scattered in 4 columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. The image_predictions dataset also has values represented as variables**\n",
    "#### Define\n",
    "The ***`image_predictions`*** dataset. Values represented as variables. \n",
    "Melting the values columns into real variable columns using the **`pd.wide_to_long`** function after renaming the columns to be more descriptive and facilitate the fuction use.\n",
    "\n",
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Original names\n",
    "image_clean.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_clean.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming the dataset columns\n",
    "cols = ['tweet_id', 'jpg_url', 'img_num', \n",
    "       'prediction_1', 'confidence_1', 'dog_1',\n",
    "       'prediction_2', 'confidence_2', 'dog_2',\n",
    "       'prediction_3', 'confidence_3', 'dog_3']\n",
    "image_clean.columns = cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping the dataframe\n",
    "image_clean = pd.wide_to_long(image_clean,\n",
    "                              stubnames=['prediction', 'confidence', 'dog'], \n",
    "    i=['tweet_id', 'jpg_url', 'img_num'], j='prediction_level', sep=\"_\")\\\n",
    ".reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the new shape\n",
    "image_clean.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_clean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the overall number of rows has been tripled\n",
    "6225/2075"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Resources: \n",
    "-  [Pandas Melt several groups of columns into multiple target columns by name](https://stackoverflow.com/questions/38862832/pandas-melt-several-groups-of-columns-into-multiple-target-columns-by-name)\n",
    "-  [pandas.wide_to_long](https://pandas.pydata.org/pandasdocs/stable/reference/api/pandas.wide_to_long.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. The** ***`api_df`*** **dataset better be merged with the `archive` datasert**\n",
    "#### Define\n",
    "Using the pd.merge function to merge the two datasets\n",
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_clean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_clean = pd.merge(left=archive_clean, right=api_clean, how='left', left_on='tweet_id', right_on='id')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_clean.drop(columns=['id'], inplace=True)\n",
    "archive_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_clean.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Addressing the Quality aspects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Tweets with no photos should be removed**\n",
    "#### Define\n",
    "Use the `iamge_prediction` table to guide the selection and removal of tweets without photos in the `archive` table\n",
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a list of tweet_ids with images \"tweets_with_image\" and confirming its length\n",
    "tweets_with_image = list(image_clean.tweet_id.unique())\n",
    "len(tweets_with_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirming that all the tweets with images exist in the archive dataset\n",
    "archive_clean.tweet_id.isin(tweets_with_image).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning in action ;)\n",
    "archive_clean = archive_clean[archive_clean.tweet_id.isin(tweets_with_image)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# archive_clean structure and count verification\n",
    "archive_clean.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Retweets and replies removal**\n",
    "### Define\n",
    "In the follwing part, the following redundant columns **`(in_reply_to_status_id, in_reply_to_user_id, 'retweeted_status_id', 'retweeted_status_user_id', 'retweeted_status_timestamp')`** will be utilized to shed the retweet and replies from our datasets and then will be dropped.\n",
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Throwback to the archive dataset to extract the tweets that include data in the retweet_status_id.\n",
    "retweet_entries = archive_clean.retweeted_status_id.notnull()\n",
    "archive_clean[retweet_entries].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the retweets from the archive data set\n",
    "archive_clean = archive_clean[~retweet_entries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting replies entries with the same method as the retweets\n",
    "reply_entries = archive_clean.in_reply_to_status_id.notnull()\n",
    "reply_entries.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the replies from the archive datafarme\n",
    "archive_clean = archive_clean[~reply_entries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the redundant columns\n",
    "archive_clean.drop(columns=['in_reply_to_status_id', \n",
    "                    'in_reply_to_user_id', \n",
    "                    'retweeted_status_id', \n",
    "                    'retweeted_status_user_id', \n",
    "                    'retweeted_status_timestamp'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we check the image_prediction table for extra tweet ids not in the archive table\n",
    "np.logical_not(image_clean.tweet_id.isin(list(archive_clean.tweet_id))).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we detected 312 entries with ids that belong to either retweets or replies taking into consideration that this figure is actually triple the actual number of the retweets and replies combined in the image prediction dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the retweets and replies ids from the image prediction dataframe\n",
    "image_clean = image_clean[~np.logical_not(image_clean.tweet_id.isin(list(archive_clean.tweet_id)))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# archive dataset counts and structure:\n",
    "archive_clean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifying the above number of records in the archive dataframe\n",
    "2075 - 81 - 23"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exactly ***104 entries*** belonging to retweets and replies have been removed successfully. Triple that number should have been removed from the image prediction table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_clean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verifying the above number of records of the image_clean table\n",
    "(6225 - 5913)/3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Inconsistent representation of null values as \"\" empty-strings in the (dog_stage) column and some entries have two stages that i will separate by a hyphen for readability.**\n",
    "\n",
    "#### Define\n",
    "replacing the empty strings in the dog_stage column of the archive dataset with pythonic null values and separating the stacked stages with a hyphen by simple pandas assignment operation\n",
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the values before cleaning\n",
    "archive_clean.dog_stage.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixing the empty string issue for more accurate representation of vlues\n",
    "archive_clean.loc[archive_clean.dog_stage == \"\", 'dog_stage'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating the stacked stages\n",
    "archive_clean.loc[archive_clean.dog_stage == 'doggopupper', 'dog_stage'] = 'doggo-pupper'\n",
    "archive_clean.loc[archive_clean.dog_stage == 'doggopuppo', 'dog_stage'] = 'doggo-puppo'\n",
    "archive_clean.loc[archive_clean.dog_stage == 'doggofloofer', 'dog_stage'] = 'doggo-floofer'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifying the cleanes version\n",
    "archive_clean.dog_stage.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Rating numerator and rating denominator inaccurate and dubious values**\n",
    "#### Define\n",
    "Firstly, i set a definition for what dubious and inaccurate values actually are.\n",
    "> -  For the denominator; any value below or above 10 is suspected, i will get to know how many values are there and slice their records from the dataset to closely investigate their pertinent text.\n",
    "> -  For the Numerator, i suspected values below 6 and above 15 and will go through the same way of slicing their relevant tweets and investigate thier texts.\n",
    "> -  Finally, i will address some of them manually as they are considered one off occurrences and try to fix the others programmatically if nececssary\n",
    "\n",
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Throwback for the rating figures\n",
    "archive_clean.loc[:, ['rating_numerator', 'rating_denominator']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deeper look into the numerator values with their count\n",
    "archive_clean.rating_numerator.value_counts(dropna = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A closer insight into the value counts of the denominator\n",
    "archive_clean.rating_denominator.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting  slicers for the data based on the set boundaries\n",
    "odd_numerator = np.logical_or(archive_clean.rating_numerator > 15, archive_clean.rating_numerator <= 5)\n",
    "odd_denominator = np.logical_or(archive_clean.rating_denominator > 10, archive_clean.rating_denominator < 10)\n",
    "odd_numerator.sum(), odd_denominator.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are **17 odd denominator** values and **101 numerators**, let's see what the slicing of the dataframe will result in as some tweets may share oddities on both figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slicing the archive dataset based on the previous boolean slicers\n",
    "odd_ratings = archive_clean.loc[np.logical_or(odd_denominator, odd_numerator), ['text', 'rating_numerator', 'rating_denominator']]\n",
    "odd_ratings.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is evident that there some odd values on both sides of the rating figures with both of them resulting in **103 records** collectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 110)\n",
    "odd_ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**These are the problems that will be addressed:**\n",
    "- Note; this part could have been done in the assessment section, but i opted to place it here because it's diving deep into the two specific variables of the rating_denominator and rating_numerator\n",
    "- ***Group (1) Fractions***: \n",
    "* After Vetting the records, i found out that the numerators with fractions that contain decimal points aren't captuerd correctly as the digits after the decimal point is the only captured part. This can be addressed programmatically.\n",
    ">  -  Fractions examples: indexes (667 , 733 , 1664)\n",
    "\n",
    "- ***Group (2) Miscelleneous reasons***: \n",
    "* Moreover, there're 10 observations that are considered special cases to be handled manually.\n",
    ">  -  #491: daily activity \"24/7\" not a rating so to be set to Null value\n",
    ">  -  #2018: \"420/10\" and not a dog but a person's photo , to be deleted \n",
    ">  -  #1849: two ratings for two dogs (5/10, 8/10) will take the average\n",
    ">  -  #1466: two ratings for two dogs  (10/10, 5/10) will take the average\n",
    ">  -  #946:  an oulier yet a valid one as per the image rating \"1776/10\", to be deleted\n",
    ">  -  #1034: 14/10 instead of 9/11; the later is a date.\n",
    ">  -  #1130: group of 4 dogs out of 20 with a rating of 13/10 each\n",
    ">  -  #1167: the rating is 11/10 not 50/50\n",
    ">  -  #1616: 10/10 not 7/11 \n",
    ">  -  #2276: 9/10 not 1/2\n",
    "\n",
    "- ***Group (3) Packs of dogs with collective ratings***: \n",
    "- In the follwing tweets, the images display groups of dogs (after opening the link in the text), the rating here is collective for all of them, but after counting the number of dogs in each photo, i discovered that the denominator is equal to **Number of dogs** multiplied by **10**.\n",
    "- In accordance, we have two possible choices to deal with them;\n",
    "    -  Either to replace the ratings with value for each individual dog after the division of both the numerator and denominator by the number of dogs. \n",
    "    -  Or we can leave it as it's beacause the quotient of dividing the numerator by the denominator will be the same.   \n",
    "- I will go for the first choice for the sake of getting numerator and denominator figures that conform with the specific schema outlined in the project description.\n",
    ">  -  #410: a pack of 7 dogs with overall rating of 84/70  -- 12/10\n",
    ">  -  #870: a pack of 10 dogs with overall rating of 165/150 --> 11/10\n",
    ">  -  #1085: a pack of 17 dogs with overall rating of 204/170 --> 12/10\n",
    ">  -  #1193: a pack of 9 dogs with overall rating of 99/90 --> 11/10\n",
    ">  -  #1219: a pack of 8 dogs with overall rating of 80/80 --> 10/10\n",
    ">  -  #1239: a pack of 5 dogs with overall rating of 45/50 --> 9/10\n",
    ">  -  #1314: a pack of 5 dogs with overall rating of 60/50 --> 12/10\n",
    ">  -  #1396: a pack of 4 dogs with overall rating of 44/40 --> 11/10\n",
    ">  -  #1314: a pack of 5 dogs with overall rating of 60/50 --> 12/10\n",
    ">  -  #1589: a pack of 11 dogs with overall rating of 121/110 --> 11/10\n",
    ">  -  #1730: a pack of 12 dogs with overall rating of 144/120 --> 12/10\n",
    ">  -  #1793: a pack of 5 dogs with overall rating of 88/80 --> 12/10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Group (1) Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Compiling a regex to capture the decimal as well as whole numbers and testing it on some sample text from the dataframe\n",
    "num_p = re.compile('(\\d+\\.?\\d?\\d?)\\/(\\d{1,3})')\n",
    "print(re.findall(num_p, 'This is Bella. She hopes her smile made you smile. If not, she is also offering you her favorite monkey. 13.5/10 https://t.co/qjrljjt948'))\n",
    "re.findall(num_p, 'This is Logan, the Chow who lived. He solemnly swears he''s up to lots of good. H*ckin magical af 9.75/10 https://t.co/yBO5wuqaPS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the previously compiled pattern to extract the nominator in more robust way across the whole datafram\n",
    "archive_clean['rating_numerator'] = archive_clean.text.str.extract('(\\d+\\.?\\d?\\d?)\\/\\d{1,3}',expand = False).astype('float')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Group (1) Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifying that the ratings with fractions were correctly captured e.g. indeixec (667 , 733 , 1664):\n",
    "archive_clean.loc[[667 , 733 , 1664], ['text', 'rating_numerator', 'rating_denominator']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Group (2) Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#index - 491: daily activity \"24/7\" not a rating\n",
    "archive_clean.loc[[491], ['text', 'rating_numerator', 'rating_denominator']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting this tweet ratings to null\n",
    "archive_clean.loc[491, ['rating_numerator', 'rating_denominator']] =  [np.nan , np.nan] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Index - 946: An oulier yet a valid one as per the image rating \"1776/10\", to be deleted not to bias fututre analysis\n",
    "archive_clean.loc[[946], ['text', 'rating_numerator', 'rating_denominator']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping this tweet from the datafram\n",
    "archive_clean.drop(946, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index - 1034: 14/10 instead of 9/11; the later is a date.\n",
    "archive_clean.loc[[1034], ['text', 'rating_numerator', 'rating_denominator']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correcting the rating\n",
    "archive_clean.loc[1034, ['rating_numerator', 'rating_denominator']] =  [14 , 10] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index - 1130: group of 4 dogs out of 20 with a rating of 13/10 each\n",
    "archive_clean.loc[[1130], ['text', 'rating_numerator', 'rating_denominator']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with the accurate values\n",
    "archive_clean.loc[1130, ['rating_numerator', 'rating_denominator']] =  [13 , 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Index - 1167: the rating is 11/10 not 50/50\n",
    "archive_clean.loc[[1167], ['text', 'rating_numerator', 'rating_denominator']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correcting the rating\n",
    "archive_clean.loc[1167, ['rating_numerator', 'rating_denominator']] =  [11 , 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index - 1466: two ratings for two dogs (10/10, 5/10) will take the average\n",
    "archive_clean.loc[[1466], ['text', 'rating_numerator', 'rating_denominator']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the rating to the average:\n",
    "archive_clean.loc[1466, ['rating_numerator', 'rating_denominator']] =  [7.5 , 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index - 1616: 10/10 not 7/11\n",
    "archive_clean.loc[[1616], ['text', 'rating_numerator', 'rating_denominator']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correcting the values\n",
    "archive_clean.loc[1616, ['rating_numerator', 'rating_denominator']] =  [10 , 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index - 1849: two ratings for two dogs (5/10, 8/10) will take the average\n",
    "archive_clean.loc[[1849], ['text', 'rating_numerator', 'rating_denominator']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting rating to the average\n",
    "archive_clean.loc[1849, ['rating_numerator', 'rating_denominator']] =  [6.5 , 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index - 2018: \"420/10\" and not a dog but a person's photo , to be deleted \n",
    "archive_clean.loc[[2018] ,['text', 'rating_numerator', 'rating_denominator']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Giving its extreme numerator value and after checking the image that is not for a dog, i opted to exclude this tweet\n",
    "archive_clean.drop(2018, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Index - 2276: 9/10 not 1/2\n",
    "archive_clean.loc[[2276] ,['text', 'rating_numerator', 'rating_denominator']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correcting the ratings values\n",
    "archive_clean.loc[2276, ['rating_numerator', 'rating_denominator']] =  [9 , 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Group(2) Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index - 491: Confirming that this tweet rating was reolaced by null values\n",
    "archive_clean.loc[[491], ['text', 'rating_numerator', 'rating_denominator']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index - 946: verifying the deletion\n",
    "archive_clean.loc[945:948, ['text', 'rating_numerator', 'rating_denominator']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index - 1034: confriming the correction\n",
    "archive_clean.loc[[1034], ['text', 'rating_numerator', 'rating_denominator']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index - 1130: confriming the correction\n",
    "archive_clean.loc[[1130], ['text', 'rating_numerator', 'rating_denominator']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index - 1167: confriming the correction\n",
    "archive_clean.loc[[1167], ['text', 'rating_numerator', 'rating_denominator']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index - 1466: set to the average\n",
    "archive_clean.loc[[1466], ['text', 'rating_numerator', 'rating_denominator']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index - 1616: 10/10 \n",
    "archive_clean.loc[[1616], ['text', 'rating_numerator', 'rating_denominator']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index - 1616: set to average\n",
    "archive_clean.loc[[1849], ['text', 'rating_numerator', 'rating_denominator']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index 2018 is deleted\n",
    "archive_clean.loc[2017:2020, ['text', 'rating_numerator', 'rating_denominator']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Index - 2276: 9/10 not 1/2\n",
    "archive_clean.loc[[2276] ,['text', 'rating_numerator', 'rating_denominator']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Group (3) Code\n",
    "> -Via simple pandas series arithmetic operations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = [410, 870, 1085, 1193, 1219, 1239, 1314, 1396, 1589, 1730,1793]\n",
    "archive_clean.loc[indices, ['text', 'rating_numerator', 'rating_denominator']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the dogs counts in the tweets featuring many dogs\n",
    "dogs_count = archive_clean.rating_denominator[archive_clean.rating_numerator >= 40]/10\n",
    "dogs_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing the Calculations\n",
    "archive_clean.loc[archive_clean.rating_numerator >= 40, ['rating_numerator', 'rating_denominator']] = [archive_clean.rating_numerator[archive_clean.rating_numerator >= 40]/dogs_count , 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Group(3) Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirming the switch to the individual ratings figures for consistency\n",
    "archive_clean.loc[indices, ['text', 'rating_numerator', 'rating_denominator']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing for overall ratings figures\n",
    "archive_clean.loc[:, ['rating_numerator', 'rating_denominator']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_clean.rating_numerator.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_clean.rating_denominator.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- With the removal of just dubious tweets and fixing the faulty figures, the rating_numerator and rating_denominator are now ready for producing reliable analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Fixing datatypes** \n",
    "#### Define\n",
    "- The `dog_stage` in the `archive` table from string to categorical\n",
    "- The timestamp column type into datetime object using the function pd.to_datetime\n",
    "- The `prediction_level` in the `image_prediction` table from integer to categorical\n",
    "- The `tweet_id` in both table from integer to string.\n",
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datatypes in the archive table\n",
    "archive_clean.tweet_id = archive_clean.tweet_id.astype('str')\n",
    "archive_clean.dog_stage = archive_clean.dog_stage.astype('category')\n",
    "archive_clean['timestamp'] = pd.to_datetime(archive_clean.timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datatypes in the image prediction table\n",
    "image_clean.tweet_id = image_clean.tweet_id.astype('str')\n",
    "image_clean.prediction_level = image_clean.prediction_level.astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_clean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_clean.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Inconsistent entries capitalization of the `prediction` column in the `image_prediction` dataframe**\n",
    "#### Define\n",
    "Applying the string method `.capitalize` on this column\n",
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_clean.prediction = image_clean.prediction.str.capitalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_clean.prediction.value_counts().head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4- Saving the Cleaned data Locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_clean.to_csv('twitter_archive_master.csv', index=False, encoding='utf-8')\n",
    "image_clean.to_csv('image_predictions_tidy.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5- Data Insights `(Analysis and Visualization)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arc_df = pd.read_csv('twitter_archive_master.csv')\n",
    "predictions_df = pd.read_csv('image_predictions_tidy.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the characteristics of the twitter archive data after reloading\n",
    "arc_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the characteristics of the image prediction data after reloading\n",
    "predictions_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**It's clear that the datatypes have been changed during the save-reload process, restoring them by running the following code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datatypes in the arc_df\n",
    "arc_df.tweet_id = arc_df.tweet_id.astype('str')\n",
    "arc_df.dog_stage = arc_df.dog_stage.astype('category')\n",
    "arc_df['timestamp'] = pd.to_datetime(arc_df.timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datatypes in the image predictions_df\n",
    "predictions_df.tweet_id = predictions_df.tweet_id.astype('str')\n",
    "predictions_df.prediction_level = predictions_df.prediction_level.astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context(context='notebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigating Patterns and relationships in arc_df table\n",
    "pd.plotting.scatter_matrix(arc_df.iloc[:,1:], figsize=(15,15));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the scatter-matrix we can spot a positive relationship between the retweet_count and favorite_count. Also, it seems that there is some correlation between the rating numerator and both retweet_count and favorite_count."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Correlation matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arc_df.iloc[:, 1:].corr(method='pearson')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As observed from the scatter-matrix, there is a strong positive linear relationship retweet_count and favorite_count `(corr_coeff = 0.9297)`. \n",
    "- There's also a moderate positive relationship between the rating_numerator and favorite_count`(corr_coeff = 0.396)`. A weaker postive relationship between the rating_numerator and retweet_count is indicated by the `correlation coefficient = 0.299`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retweet Counts & Favorite Counts "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Scatter plot between the retweet_count and favorite_count**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context('talk')\n",
    "def r2(x,y):\n",
    "    return stats.pearsonr(x,y)[0]**2\n",
    "sns.jointplot(arc_df.favorite_count, arc_df.retweet_count, kind=\"reg\", stat_func=r2 )\n",
    "plt.tick_params(axis=\"both\", labelsize=12);\n",
    "plt.savefig(\"retweet_count_favorite_count_corr.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the relation between retweet_count and favorite_count shows with the pearson r^2 of 0.86 illustrates the strong positive relationship between these two variavbles "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resources:\n",
    "[Seaborn implot with equation and R2 text](https://stackoverflow.com/questions/25579227/seaborn-implot-with-equation-and-r2-text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arc_df.loc[:, ['retweet_count', 'favorite_count']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I created another dataframe (arc) in which the timestamp column is the index and grouped it by monthly means for smoother plotting of variables like the favorite counts and retweet counts. I such way, i removed the noise of plotting timestamp data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for plotting\n",
    "arc = pd.read_csv('twitter_archive_master.csv', index_col = 'timestamp', parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arc.index = arc.index.date\n",
    "arc.index = pd.to_datetime(arc.index, infer_datetime_format=True)\n",
    "arc = arc.resample('M').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arc.loc[:,['rating_numerator','retweet_count','favorite_count','followers_count']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Average retweet_count and favorite_count trend over time span of the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.set_context('talk')\n",
    "arc['favorite_count'].plot(style='k', figsize=(12,8), label='Favorite count');\n",
    "arc.retweet_count.plot(style='b',label='Retweet count');\n",
    "plt.legend()\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"Date\");plt.ylabel(\"Average Count\");\n",
    "plt.title('Favorites & Retweets over time')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"favorite_Retweet_count_trend.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Tweets count trend over the dataset time period**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arc_timeseries = arc_df.copy()\n",
    "arc_timeseries = arc_timeseries.set_index('timestamp')\n",
    "arc_timeseries.sort_index()\n",
    "master_timeseries = arc_timeseries.reset_index().merge(predictions_df[predictions_df.prediction_level == 1],\n",
    "         on='tweet_id', how='left').set_index(arc_timeseries.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context('notebook')\n",
    "master_timeseries.groupby([(master_timeseries.index.year), (master_timeseries.index.month)]).tweet_id.count().plot(kind='line',color='r',linestyle='--',\n",
    "marker='D',figsize=(12,4));\n",
    "mn=np.mean(master_timeseries.groupby([(master_timeseries.index.year), (master_timeseries.index.month)]).count()['tweet_id'])\n",
    "plt.axhline(y=mn, color='b', linestyle='--', label='Out of rating')\n",
    "plt.xlabel(\"Date\");plt.ylabel(\"Tweet Count\");\n",
    "plt.title('Tweets Monthly count')\n",
    "plt.savefig(\"tweets_monthly_count_trend.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. retweet_count and favorite_count for tweets featuring dogs and those not featuring dogs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_timeseries.groupby(['dog']).retweet_count.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context('poster')\n",
    "ax = sns.boxplot(x='dog', y='retweet_count', data=master_timeseries, showfliers=False)\n",
    "plt.savefig(\"boxplots_retweet_count_dogs_not_dogs.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.boxplot(x='dog', y='retweet_count',  \n",
    "                 data=master_timeseries[master_timeseries.retweet_count<7500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_timeseries.groupby(['dog']).favorite_count.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context('talk')\n",
    "ax = sns.boxplot(x='dog', y='favorite_count', data=master_timeseries, showfliers=False)\n",
    "plt.setp(ax.get_xticklabels(), rotation=45)\n",
    "plt.savefig(\"boxplots_favorite_count_dogs_not_dogs.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rating and their relationship with other variables\n",
    "**6. Rating patterns over time**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a copy of the arc_df with timestapm as index\n",
    "sns.set_context('poster')\n",
    "arc_timeseries = arc_df.copy()\n",
    "arc_timeseries.index = arc_timeseries['timestamp']\n",
    "arc_timeseries.drop(columns=['timestamp'], inplace=True)\n",
    "arc_timeseries.sort_index(inplace=True)\n",
    "arc_timeseries.groupby([(arc_timeseries.index.year),\n",
    "                       (arc_timeseries.index.month)]).rating_numerator.mean().plot(style='-ro', figsize=(12,8),label='Total average rating')\n",
    "plt.axhline(y=10.0, color='b', linestyle='--', label='Out of rating')\n",
    "plt.xlabel(\"Date rated\");\n",
    "plt.ylabel(\"Rating out of 10\");\n",
    "plt.legend();\n",
    "plt.savefig(\"Average_Rating_pattern.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7. Rating distribution for images featuring dogs and images with no dogs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context('notebook')\n",
    "plt.subplots(figsize=(6,4))\n",
    "sns.boxplot(master_timeseries.index.year, master_timeseries.rating_numerator, hue=master_timeseries.dog, palette=\"Set1\");\n",
    "plt.legend(loc=8)\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(0.5, 1.2)  ,ncol=3, fancybox=True, shadow=True)\n",
    "plt.xlabel(\"Rating Year\"); plt.ylabel(\"Rating\");\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"Rating_per_year_for_dogs_no_dogs.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resources**: [How to keep index when using pandas merge](https://stackoverflow.com/questions/11976503/how-to-keep-index-when-using-pandas-merge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8. Dog Stage distribution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context('poster')\n",
    "ordered_stages = arc_df['dog_stage'].value_counts().head(4).index\n",
    "sns.countplot(data = arc_df, x = 'dog_stage', order=ordered_stages ,orient = 'h')\n",
    "plt.title('Dog stages Distribution',fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"Dog_stage_distribution.png\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context('talk')\n",
    "ax = sns.boxplot(x='dog_stage', y='favorite_count', showfliers=False, width = .70,\n",
    "                 data=master_timeseries)\n",
    "sns.set(rc={'figure.figsize':(8.0,16.0)})\n",
    "plt.savefig(\"Dog_stage_favorite_count.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context('talk')\n",
    "ax = sns.boxplot(x='dog_stage', y='retweet_count', showfliers=False, width = .70,\n",
    "                 data=master_timeseries)\n",
    "sns.set(rc={'figure.figsize':(15.5, 7.5)});\n",
    "plt.savefig(\"Dog_stage_retweet_count.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the large amount of missing data concerning  this variable, This plot shows that the pupper stage is the most prevalent stage. This may be attributed to the fact that younger puppies are usually more attractive. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**9. Glimpse on Breeds** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context('talk')\n",
    "master_timeseries[master_timeseries.dog == True].prediction.value_counts()[12::-1].plot(kind='barh')\n",
    "plt.xlabel('tweets_count')\n",
    "plt.ylabel('Dog Breed');\n",
    "plt.savefig(\"Top_Breeds_Distribution.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context('talk')\n",
    "master_timeseries[master_timeseries.dog == True].groupby(['prediction']).retweet_count.sum().sort_values(ascending=False)[12::-1].plot(kind='barh');\n",
    "plt.xlabel('Retweets_count')\n",
    "plt.ylabel('Dog Breed');\n",
    "plt.savefig(\"Top_Breeds_Retweet_Count.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context('talk')\n",
    "master_timeseries[master_timeseries.dog == True].groupby(['prediction']).favorite_count.sum().sort_values(ascending=False)[12::-1].plot(kind='barh');\n",
    "plt.xlabel('favorites_count')\n",
    "plt.ylabel('Dog Breed');\n",
    "plt.savefig(\"Top_Breeds_Favorite_count.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**10. Followers Insight**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(12, 8))\n",
    "plt.plot(arc.followers_count)\n",
    "plt.xlabel('\\nDate (YYYY-MM)')\n",
    "plt.ylabel('Number of Followers\\n');\n",
    "plt.savefig('followers_decline.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will stop here, yet definitly there are a ton of insights to glean and analyse...!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
